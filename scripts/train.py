import argparse
import sys
import os
import json
import time
import signal
import threading
import psutil
import random
import numpy as np
from pathlib import Path

os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

from ultralytics import YOLO

GPU_MONITOR_INTERVAL = 2.0
GPU_MEMORY_WARNING_THRESHOLD = 0.85
GPU_UTIL_WARNING_THRESHOLD = 0.30

PARENT_PID = os.getppid()
CHECK_INTERVAL = 10
should_stop = False

def log_json(data):
    print(f"__JSON_LOG__{json.dumps(data)}", flush=True)

def check_parent_alive():
    """å®šæœŸæ£€æŸ¥çˆ¶è¿›ç¨‹æ˜¯å¦å­˜æ´»ï¼Œå¦‚æœçˆ¶è¿›ç¨‹å·²é€€å‡ºåˆ™è‡ªåŠ¨ç»ˆæ­¢è‡ªèº«"""
    global should_stop
    try:
        if PARENT_PID == 1:
            print("âš ï¸ çˆ¶è¿›ç¨‹ä¸º init (PID=1)ï¼Œå‡è®¾çˆ¶è¿›ç¨‹å·²é€€å‡º", flush=True)
            should_stop = True
            return False
        
        try:
            parent = psutil.Process(PARENT_PID)
            if not parent.is_running():
                print(f"âš ï¸ çˆ¶è¿›ç¨‹ (PID={PARENT_PID}) ä¸å†è¿è¡Œï¼Œæ­£åœ¨é€€å‡º...", flush=True)
                should_stop = True
                return False
        except psutil.NoSuchProcess:
            print(f"âš ï¸ çˆ¶è¿›ç¨‹ (PID={PARENT_PID}) ä¸å­˜åœ¨ï¼Œæ­£åœ¨é€€å‡º...", flush=True)
            should_stop = True
            return False
        except psutil.AccessDenied:
            pass
            
        return True
    except Exception as e:
        print(f"âš ï¸ æ£€æŸ¥çˆ¶è¿›ç¨‹çŠ¶æ€æ—¶å‡ºé”™: {e}", flush=True)
        return True

def parent_monitor_thread():
    """åå°çº¿ç¨‹ï¼šå®šæœŸæ£€æŸ¥çˆ¶è¿›ç¨‹æ˜¯å¦å­˜æ´»"""
    global should_stop
    print(f"ğŸ”„ çˆ¶è¿›ç¨‹ç›‘æ§çº¿ç¨‹å·²å¯åŠ¨ (çˆ¶è¿›ç¨‹ PID: {PARENT_PID}, æ£€æŸ¥é—´éš”: {CHECK_INTERVAL}s)", flush=True)
    
    while not should_stop:
        time.sleep(CHECK_INTERVAL)
        if should_stop:
            break
        if not check_parent_alive():
            log_json({
                "event": "parent_exit",
                "message": "çˆ¶è¿›ç¨‹å·²é€€å‡ºï¼Œè®­ç»ƒå³å°†åœæ­¢"
            })
            break
    
    print("ğŸ”„ çˆ¶è¿›ç¨‹ç›‘æ§çº¿ç¨‹å·²é€€å‡º", flush=True)

def signal_handler(signum, frame):
    """å¤„ç†ç»ˆæ­¢ä¿¡å·"""
    global should_stop
    print(f"æ”¶åˆ°ä¿¡å· {signum}ï¼Œæ­£åœ¨åœæ­¢è®­ç»ƒ...", flush=True)
    should_stop = True

signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

ERROR_MAPPINGS = {
    "cuda": {
        "keywords": ["cuda", "CUDA", "GPU", "gpu device"],
        "type": "hardware",
        "title": "GPU ç›¸å…³é”™è¯¯",
        "suggestions": [
            "è¯·ç¡®è®¤å·²æ­£ç¡®å®‰è£… NVIDIA æ˜¾å¡é©±åŠ¨",
            "æ£€æŸ¥ PyTorch æ˜¯å¦æ”¯æŒ CUDA",
            "å°è¯•å°† device å‚æ•°æ”¹ä¸º 'cpu' ä½¿ç”¨ CPU æ¨¡å¼è®­ç»ƒ"
        ]
    },
    "cuda_oom": {
        "keywords": ["out of memory", "OOM", "CUDA out of memory", "RuntimeError: CUDA out of memory"],
        "type": "oom",
        "title": "æ˜¾å­˜ä¸è¶³ (OOM)",
        "suggestions": [
            "å‡å° batch_size å‚æ•°ï¼ˆå½“å‰å€¼å¯èƒ½è¿‡å¤§ï¼‰",
            "å‡å° imgsz å›¾ç‰‡å°ºå¯¸",
            "å°è¯•ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆå¦‚ yolov8n æˆ– yolov8sï¼‰",
            "å¼€å¯æ··åˆç²¾åº¦è®­ç»ƒå¯å‡å°‘æ˜¾å­˜å ç”¨"
        ]
    },
    "cudnn": {
        "keywords": ["cudnn", "CUDNN", "cuDNN"],
        "type": "cudnn",
        "title": "cuDNN é”™è¯¯",
        "suggestions": [
            "å¯èƒ½æ˜¯ CUDA ç‰ˆæœ¬ä¸ cuDNN ä¸åŒ¹é…",
            "å°è¯•æ›´æ–° NVIDIA é©±åŠ¨åˆ°æœ€æ–°ç‰ˆæœ¬",
            "æˆ–è®¾ç½®ç¯å¢ƒå˜é‡ CUDA_LAUNCH_BLOCKING=1 è·å–æ›´å¤šè°ƒè¯•ä¿¡æ¯"
        ]
    },
    "torch_cuda": {
        "keywords": ["torch.cuda", "torch is not able to use GPU"],
        "type": "torch_cuda",
        "title": "PyTorch æ— æ³•ä½¿ç”¨ GPU",
        "suggestions": [
            "ç¡®è®¤ PyTorch å·²æ­£ç¡®å®‰è£… CUDA ç‰ˆæœ¬",
            "æ‰§è¡Œ python -c \"import torch; print(torch.cuda.is_available())\" æ£€æŸ¥",
            "å°è¯•é‡æ–°å®‰è£… PyTorch: pip install torch --index-url https://download.pytorch.org/whl/cu118"
        ]
    },
    "no_gpu": {
        "keywords": ["No CUDA GPUs are available", "No GPU detected"],
        "type": "no_gpu",
        "title": "æœªæ£€æµ‹åˆ°å¯ç”¨ GPU",
        "suggestions": [
            "è¯·ç¡®è®¤ç”µè„‘å·²å®‰è£… NVIDIA æ˜¾å¡",
            "æ£€æŸ¥æ˜¾å¡é©±åŠ¨æ˜¯å¦æ­£ç¡®å®‰è£…",
            "åœ¨è®¾å¤‡ç®¡ç†å™¨ä¸­ç¡®è®¤æ˜¾å¡æœªè¢«ç¦ç”¨",
            "å¯ä½¿ç”¨ device: 'cpu' ä½¿ç”¨ CPU è¿›è¡Œè®­ç»ƒ"
        ]
    },
    "memory": {
        "keywords": ["MemoryError", "cannot allocate memory", "Unable to allocate"],
        "type": "memory",
        "title": "å†…å­˜ä¸è¶³",
        "suggestions": [
            "ç³»ç»Ÿå†…å­˜ä¸è¶³ï¼Œå°è¯•å…³é—­å…¶ä»–ç¨‹åº",
            "å‡å° batch_size å‚æ•°",
            "æ£€æŸ¥æ˜¯å¦æœ‰å†…å­˜æ³„æ¼"
        ]
    }
}

def detect_hardware():
    """è½»é‡çº§ç¡¬ä»¶ç¯å¢ƒæ£€æµ‹æ¨¡å—ï¼ŒéªŒè¯ GPU å¯ç”¨æ€§"""
    print("ğŸ” æ­£åœ¨æ£€æµ‹ç¡¬ä»¶ç¯å¢ƒ...", flush=True)
    
    result = {
        "available": False,
        "gpu_count": 0,
        "gpu_name": None,
        "cuda_version": None,
        "torch_cuda_version": None,
        "errors": []
    }
    
    try:
        import torch
        result["torch_cuda_version"] = torch.version.cuda if torch.version.cuda else None
        
        if torch.cuda.is_available():
            result["available"] = True
            result["gpu_count"] = torch.cuda.device_count()
            result["gpu_name"] = torch.cuda.get_device_name(0) if result["gpu_count"] > 0 else None
            result["cuda_version"] = torch.version.cuda
            
            total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            result["total_memory_gb"] = round(total_memory, 2)
            
            print(f"âœ… æ£€æµ‹åˆ° {result['gpu_count']} ä¸ª GPU", flush=True)
            print(f"   GPU 0: {result['gpu_name']}", flush=True)
            print(f"   CUDA ç‰ˆæœ¬: {result['cuda_version']}", flush=True)
            print(f"   æ˜¾å­˜æ€»é‡: {result['total_memory_gb']} GB", flush=True)
        else:
            result["errors"].append("CUDA ä¸å¯ç”¨")
            print("âš ï¸ CUDA ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ CPU æ¨¡å¼", flush=True)
            
    except ImportError:
        result["errors"].append("PyTorch æœªå®‰è£…")
        print("âš ï¸ PyTorch æœªå®‰è£…", flush=True)
    except Exception as e:
        result["errors"].append(str(e))
        print(f"âš ï¸ ç¡¬ä»¶æ£€æµ‹å¼‚å¸¸: {e}", flush=True)
    
    return result

def classify_error(error_msg):
    """æ ¹æ®é”™è¯¯ä¿¡æ¯åˆ†ç±»å¹¶è¿”å›å¤„ç†å»ºè®®"""
    error_lower = error_msg.lower()
    
    for key, mapping in ERROR_MAPPINGS.items():
        for keyword in mapping["keywords"]:
            if keyword.lower() in error_lower:
                return {
                    "type": mapping["type"],
                    "title": mapping["title"],
                    "suggestions": mapping["suggestions"],
                    "raw_error": error_msg[:500]
                }
    
    return {
        "type": "unknown",
        "title": "è®­ç»ƒé”™è¯¯",
        "suggestions": ["è¯·æ£€æŸ¥é…ç½®å‚æ•°æ˜¯å¦æ­£ç¡®", "æŸ¥çœ‹ä¸‹æ–¹åŸå§‹é”™è¯¯ä¿¡æ¯"],
        "raw_error": error_msg[:500]
    }

def export_model(model, args, model_path):
    """è®­ç»ƒå®Œæˆåè‡ªåŠ¨å¯¼å‡ºæ¨¡å‹åˆ°å¤šç§æ ¼å¼"""
    try:
        export_formats = args.export_formats.split(',') if isinstance(args.export_formats, str) else args.export_formats
        
        print(f"ğŸ“¦ å°†å¯¼å‡ºä»¥ä¸‹æ ¼å¼: {', '.join(export_formats)}")
        
        for pkg in ['pandas', 'matplotlib']:
            try:
                __import__(pkg)
            except ImportError:
                print(f"âš ï¸ è­¦å‘Š: {pkg} æœªå®‰è£…ï¼Œå¯¼å‡ºå’Œç»˜å›¾åŠŸèƒ½å¯èƒ½å—é™", flush=True)
                print(f"   å®‰è£…å‘½ä»¤: pip install {pkg}", flush=True)
        
        results = {
            "event": "export_start",
            "formats": export_formats,
            "base_model": model_path
        }
        
        exported_files = []
        failed_formats = []
        
        for fmt in export_formats:
            fmt = fmt.strip().lower()
            print(f"   æ­£åœ¨å¯¼å‡º {fmt} æ ¼å¼...", flush=True)
            
            try:
                export_path = model.export(format=fmt)
                
                if isinstance(export_path, str):
                    exported_files.append(export_path)
                    print(f"   âœ… {fmt} å¯¼å‡ºæˆåŠŸ: {export_path}", flush=True)
                else:
                    exported_files.append(str(export_path))
                    print(f"   âœ… {fmt} å¯¼å‡ºæˆåŠŸ", flush=True)
                    
            except Exception as e:
                failed_formats.append(fmt)
                print(f"   âŒ {fmt} å¯¼å‡ºå¤±è´¥: {e}", flush=True)
        
        print(f"\nâœ… æ¨¡å‹å¯¼å‡ºå®Œæˆ!", flush=True)
        
        final_result = {
            "event": "export_complete",
            "exported": exported_files,
            "failed": failed_formats,
            "total": len(export_formats),
            "success_count": len(exported_files),
            "failed_count": len(failed_formats)
        }
        
        log_json(final_result)
        
        if failed_formats:
            print(f"\nâš ï¸ ä»¥ä¸‹æ ¼å¼å¯¼å‡ºå¤±è´¥: {', '.join(failed_formats)}", flush=True)
        
        return final_result
        
    except Exception as e:
        print(f"âš ï¸ æ¨¡å‹å¯¼å‡ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", file=sys.stderr)
        log_json({
            "event": "export_error",
            "message": str(e),
            "base_model": model_path
        })
        return None

def validate_model(model, args, model_path):
    """è®­ç»ƒå®Œæˆåæ‰§è¡Œæ¨¡å‹éªŒè¯ï¼Œç”Ÿæˆè¯„ä¼°æŒ‡æ ‡ã€æ··æ·†çŸ©é˜µå’Œ PR æ›²çº¿"""
    try:
        val_results = model.val(
            data=args.data,
            batch=args.batch,
            imgsz=args.imgsz,
            device=args.device,
            save_json=True,
            save_hybrid=True,
            plots=True
        )
        
        print("âœ… æ¨¡å‹éªŒè¯å®Œæˆï¼")
        
        validation_data = {
            "event": "validation_complete",
            "model_path": model_path,
            "metrics": {}
        }
        
        # æå–ä¸»è¦è¯„ä¼°æŒ‡æ ‡
        if hasattr(val_results, 'box'):
            box_metrics = val_results.box
            validation_data["metrics"] = {
                "mAP50": float(getattr(box_metrics, 'map50', 0)),
                "mAP50-95": float(getattr(box_metrics, 'map', 0)),
                "precision": float(getattr(box_metrics, 'mp', 0)),
                "recall": float(getattr(box_metrics, 'mr', 0)),
                "f1": float(getattr(box_metrics, 'mf', 0)) if hasattr(box_metrics, 'mf') else 0.0
            }
        
        if hasattr(val_results, 'pose'):
            pose_metrics = val_results.pose
            validation_data["metrics"]["pose_mAP50"] = float(getattr(pose_metrics, 'map50', 0)) if hasattr(pose_metrics, 'map50') else 0.0
            validation_data["metrics"]["pose_mAP50-95"] = float(getattr(pose_metrics, 'map', 0)) if hasattr(pose_metrics, 'map') else 0.0
        
        # ç”Ÿæˆæ··æ·†çŸ©é˜µå’Œ PR æ›²çº¿çš„è·¯å¾„
        results_dir = os.path.join(args.project, args.name)
        
        confusion_matrix_path = os.path.join(results_dir, 'confusion_matrix.png')
        pr_curve_path = os.path.join(results_dir, 'PR_curve.png')
        
        validation_data["artifacts"] = {
            "confusion_matrix": confusion_matrix_path if os.path.exists(confusion_matrix_path) else None,
            "pr_curve": pr_curve_path if os.path.exists(pr_curve_path) else None
        }
        
        # æ‰“å°è¯„ä¼°æŒ‡æ ‡
        metrics = validation_data["metrics"]
        print(f"ğŸ“Š éªŒè¯æŒ‡æ ‡:")
        print(f"   mAP@50: {metrics.get('mAP50', 0):.4f}")
        print(f"   mAP@50-95: {metrics.get('mAP50-95', 0):.4f}")
        print(f"   Precision: {metrics.get('precision', 0):.4f}")
        print(f"   Recall: {metrics.get('recall', 0):.4f}")
        
        if metrics.get('pose_mAP50'):
            print(f"   Pose mAP@50: {metrics.get('pose_mAP50', 0):.4f}")
        
        # å‘é€éªŒè¯ç»“æœåˆ°å‰ç«¯
        log_json(validation_data)
        
        return validation_data
        
    except Exception as e:
        print(f"âš ï¸ æ¨¡å‹éªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", file=sys.stderr)
        log_json({
            "event": "validation_error",
            "message": str(e),
            "model_path": model_path
        })
        return None

def format_user_friendly_error(error_msg):
    """å°†æŠ€æœ¯é”™è¯¯è½¬æ¢ä¸ºç”¨æˆ·å‹å¥½çš„é”™è¯¯ä¿¡æ¯"""
    classified = classify_error(error_msg)
    
    return {
        "event": "error",
        "type": classified["type"],
        "title": classified["title"],
        "message": error_msg[:300],
        "suggestions": classified["suggestions"],
        "action": "è¯·æ ¹æ®å»ºè®®è°ƒæ•´åé‡è¯•"
    }

class GPUMonitor:
    def __init__(self, device_id=0):
        self.device_id = device_id
        self.running = False
        self.monitor_thread = None
        self.latest_stats = {
            "gpu_memory_used_gb": 0.0,
            "gpu_memory_total_gb": 0.0,
            "gpu_memory_percent": 0.0,
            "gpu_utilization_percent": 0.0,
            "gpu_temperature": 0,
            "gpu_power_draw": 0.0,
            "gpu_power_limit": 0.0,
            "warnings": []
        }
        self.history = []
        self.max_history = 100
        self.pynvml_available = False
        self._init_pynvml()
    
    def _init_pynvml(self):
        try:
            import pynvml
            pynvml.nvmlInit()
            self.pynvml = pynvml
            self.handle = pynvml.nvmlDeviceGetHandleByIndex(self.device_id)
            self.pynvml_available = True
            print(f"âœ… GPUç›‘æ§å·²åˆå§‹åŒ– (è®¾å¤‡ {self.device_id})", flush=True)
        except ImportError:
            print("âš ï¸ pynvmlæœªå®‰è£…ï¼ŒGPUç›‘æ§åŠŸèƒ½å—é™ã€‚å®‰è£…: pip install pynvml", flush=True)
        except Exception as e:
            print(f"âš ï¸ GPUç›‘æ§åˆå§‹åŒ–å¤±è´¥: {e}", flush=True)
    
    def get_gpu_stats(self):
        stats = {
            "gpu_memory_used_gb": 0.0,
            "gpu_memory_total_gb": 0.0,
            "gpu_memory_percent": 0.0,
            "gpu_utilization_percent": 0.0,
            "gpu_temperature": 0,
            "gpu_power_draw": 0.0,
            "gpu_power_limit": 0.0,
            "warnings": []
        }
        
        if self.pynvml_available:
            try:
                mem_info = self.pynvml.nvmlDeviceGetMemoryInfo(self.handle)
                stats["gpu_memory_used_gb"] = round(mem_info.used / (1024**3), 2)
                stats["gpu_memory_total_gb"] = round(mem_info.total / (1024**3), 2)
                stats["gpu_memory_percent"] = round(mem_info.used / mem_info.total * 100, 1)
                
                util_info = self.pynvml.nvmlDeviceGetUtilizationRates(self.handle)
                stats["gpu_utilization_percent"] = util_info.gpu
                
                try:
                    stats["gpu_temperature"] = self.pynvml.nvmlDeviceGetTemperature(self.handle, self.pynvml.NVML_TEMPERATURE_GPU)
                except:
                    pass
                
                try:
                    power_info = self.pynvml.nvmlDeviceGetPowerUsage(self.handle)
                    stats["gpu_power_draw"] = round(power_info / 1000.0, 1)
                    power_limit = self.pynvml.nvmlDeviceGetPowerManagementLimit(self.handle)
                    stats["gpu_power_limit"] = round(power_limit / 1000.0, 1)
                except:
                    pass
                
                if stats["gpu_memory_percent"] > GPU_MEMORY_WARNING_THRESHOLD * 100:
                    stats["warnings"].append(f"æ˜¾å­˜ä½¿ç”¨ç‡è¿‡é«˜: {stats['gpu_memory_percent']:.1f}%")
                
                if stats["gpu_utilization_percent"] < GPU_UTIL_WARNING_THRESHOLD * 100:
                    stats["warnings"].append(f"GPUåˆ©ç”¨ç‡ä½: {stats['gpu_utilization_percent']:.1f}% (å¯èƒ½å­˜åœ¨IOç“¶é¢ˆ)")
                
            except Exception as e:
                stats["warnings"].append(f"GPUç›‘æ§é”™è¯¯: {str(e)[:50]}")
        
        try:
            import torch
            if torch.cuda.is_available():
                stats["torch_memory_allocated_gb"] = round(torch.cuda.memory_allocated(self.device_id) / (1024**3), 3)
                stats["torch_memory_reserved_gb"] = round(torch.cuda.memory_reserved(self.device_id) / (1024**3), 3)
        except:
            pass
        
        return stats
    
    def start_monitoring(self):
        if not self.pynvml_available:
            return
        
        self.running = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self.monitor_thread.start()
        print("ğŸ”„ GPUç›‘æ§çº¿ç¨‹å·²å¯åŠ¨", flush=True)
    
    def _monitor_loop(self):
        while self.running:
            stats = self.get_gpu_stats()
            self.latest_stats = stats
            self.history.append({
                "time": time.time(),
                **stats
            })
            if len(self.history) > self.max_history:
                self.history = self.history[-self.max_history:]
            
            if stats["warnings"]:
                log_json({
                    "event": "gpu_warning",
                    "warnings": stats["warnings"],
                    "stats": {k: v for k, v in stats.items() if k != "warnings"}
                })
            
            time.sleep(GPU_MONITOR_INTERVAL)
    
    def stop_monitoring(self):
        self.running = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=2)
        print("ğŸ”„ GPUç›‘æ§çº¿ç¨‹å·²åœæ­¢", flush=True)
    
    def get_summary(self):
        if not self.history:
            return self.latest_stats
        
        mem_values = [h["gpu_memory_percent"] for h in self.history if h.get("gpu_memory_percent")]
        util_values = [h["gpu_utilization_percent"] for h in self.history if h.get("gpu_utilization_percent")]
        
        return {
            **self.latest_stats,
            "avg_memory_percent": round(np.mean(mem_values), 1) if mem_values else 0,
            "max_memory_percent": round(max(mem_values), 1) if mem_values else 0,
            "avg_utilization_percent": round(np.mean(util_values), 1) if util_values else 0,
            "monitoring_duration_samples": len(self.history)
        }

class PerformanceBenchmark:
    def __init__(self, model, device='0', imgsz=640):
        self.model = model
        self.device = device
        self.imgsz = imgsz
        self.results = {}
    
    def measure_inference_latency(self, num_runs=50, warmup=5):
        import torch
        import numpy as np
        
        print(f"â±ï¸ å¼€å§‹æ¨ç†å»¶è¿Ÿæµ‹è¯• (é¢„çƒ­: {warmup}, æµ‹è¯•: {num_runs})", flush=True)
        
        dummy_input = torch.zeros((1, 3, self.imgsz, self.imgsz))
        if torch.cuda.is_available() and self.device != 'cpu':
            dummy_input = dummy_input.cuda()
        
        for _ in range(warmup):
            try:
                _ = self.model.predict(dummy_input, verbose=False)
            except:
                pass
        
        if torch.cuda.is_available() and self.device != 'cpu':
            torch.cuda.synchronize()
        
        latencies = []
        preprocess_times = []
        inference_times = []
        postprocess_times = []
        
        for i in range(num_runs):
            start_total = time.perf_counter()
            
            preprocess_start = time.perf_counter()
            preprocess_end = time.perf_counter()
            
            if torch.cuda.is_available() and self.device != 'cpu':
                torch.cuda.synchronize()
            
            inference_start = time.perf_counter()
            try:
                _ = self.model.predict(dummy_input, verbose=False)
            except:
                pass
            
            if torch.cuda.is_available() and self.device != 'cpu':
                torch.cuda.synchronize()
            
            inference_end = time.perf_counter()
            
            postprocess_start = time.perf_counter()
            postprocess_end = time.perf_counter()
            
            total_time = (postprocess_end - start_total) * 1000
            latencies.append(total_time)
            
            preprocess_times.append((preprocess_end - preprocess_start) * 1000)
            inference_times.append((inference_end - inference_start) * 1000)
            postprocess_times.append((postprocess_end - postprocess_start) * 1000)
        
        self.results["latency"] = {
            "mean_ms": round(np.mean(latencies), 2),
            "std_ms": round(np.std(latencies), 2),
            "min_ms": round(np.min(latencies), 2),
            "max_ms": round(np.max(latencies), 2),
            "p50_ms": round(np.percentile(latencies, 50), 2),
            "p95_ms": round(np.percentile(latencies, 95), 2),
            "p99_ms": round(np.percentile(latencies, 99), 2),
            "preprocess_mean_ms": round(np.mean(preprocess_times), 2),
            "inference_mean_ms": round(np.mean(inference_times), 2),
            "postprocess_mean_ms": round(np.mean(postprocess_times), 2)
        }
        
        print(f"   å¹³å‡å»¶è¿Ÿ: {self.results['latency']['mean_ms']:.2f}ms (P95: {self.results['latency']['p95_ms']:.2f}ms)", flush=True)
        
        return self.results["latency"]
    
    def measure_throughput(self, batch_sizes=[1, 2, 4, 8], num_runs=30):
        import torch
        import numpy as np
        
        print(f"ğŸš€ å¼€å§‹ååé‡æµ‹è¯• (Batch Sizes: {batch_sizes})", flush=True)
        
        throughput_results = {}
        
        for batch_size in batch_sizes:
            try:
                dummy_input = torch.zeros((batch_size, 3, self.imgsz, self.imgsz))
                if torch.cuda.is_available() and self.device != 'cpu':
                    dummy_input = dummy_input.cuda()
                
                for _ in range(5):
                    try:
                        _ = self.model.predict(dummy_input, verbose=False)
                    except:
                        pass
                
                if torch.cuda.is_available() and self.device != 'cpu':
                    torch.cuda.synchronize()
                
                times = []
                for _ in range(num_runs):
                    start = time.perf_counter()
                    try:
                        _ = self.model.predict(dummy_input, verbose=False)
                    except:
                        continue
                    
                    if torch.cuda.is_available() and self.device != 'cpu':
                        torch.cuda.synchronize()
                    
                    end = time.perf_counter()
                    times.append(end - start)
                
                if times:
                    avg_time = np.mean(times)
                    fps = batch_size / avg_time
                    throughput_results[batch_size] = {
                        "batch_size": batch_size,
                        "avg_time_s": round(avg_time, 4),
                        "fps": round(fps, 1),
                        "fps_per_image": round(1 / avg_time, 1)
                    }
                    print(f"   Batch {batch_size}: {fps:.1f} FPS ({1/avg_time:.1f} FPS/image)", flush=True)
                
                del dummy_input
                if torch.cuda.is_available() and self.device != 'cpu':
                    torch.cuda.empty_cache()
                    
            except Exception as e:
                print(f"   Batch {batch_size}: æµ‹è¯•å¤±è´¥ - {str(e)[:50]}", flush=True)
                throughput_results[batch_size] = {"error": str(e)[:100]}
        
        self.results["throughput"] = throughput_results
        return throughput_results
    
    def get_realtime_fps(self):
        if "latency" not in self.results:
            return 0
        mean_latency_ms = self.results["latency"]["mean_ms"]
        if mean_latency_ms > 0:
            return round(1000 / mean_latency_ms, 1)
        return 0
    
    def get_summary(self):
        return {
            "latency": self.results.get("latency", {}),
            "throughput": self.results.get("throughput", {}),
            "realtime_fps": self.get_realtime_fps(),
            "meets_realtime_requirement": self.get_realtime_fps() >= 25
        }

class VisualValidator:
    def __init__(self, model, data_yaml, output_dir, num_samples=3):
        self.model = model
        self.data_yaml = data_yaml
        self.output_dir = Path(output_dir)
        self.num_samples = num_samples
        self.val_images = []
        self._load_val_images()
    
    def _load_val_images(self):
        try:
            import yaml
            with open(self.data_yaml, 'r', encoding='utf-8') as f:
                data_config = yaml.safe_load(f)
            
            base_path = data_config.get('path', '')
            val_path = data_config.get('val', '')
            
            if not os.path.isabs(val_path):
                val_path = os.path.join(base_path, val_path)
            
            if os.path.exists(val_path):
                image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp')
                self.val_images = [
                    os.path.join(val_path, f) 
                    for f in os.listdir(val_path) 
                    if f.lower().endswith(image_extensions)
                ]
                print(f"âœ… åŠ è½½äº† {len(self.val_images)} å¼ éªŒè¯å›¾ç‰‡ç”¨äºå¯è§†åŒ–", flush=True)
        except Exception as e:
            print(f"âš ï¸ åŠ è½½éªŒè¯å›¾ç‰‡å¤±è´¥: {e}", flush=True)
    
    def select_representative_samples(self, predictions):
        if len(self.val_images) <= self.num_samples:
            return self.val_images
        
        samples = {
            "high_confidence": None,
            "medium_confidence": None,
            "low_confidence": None
        }
        
        confidences = []
        for img_path in self.val_images[:min(50, len(self.val_images))]:
            try:
                results = self.model.predict(img_path, verbose=False)
                if results and len(results) > 0:
                    boxes = results[0].boxes
                    if boxes is not None and len(boxes) > 0:
                        conf = boxes.conf.max().item()
                        confidences.append((img_path, conf))
            except:
                pass
        
        if confidences:
            confidences.sort(key=lambda x: x[1], reverse=True)
            
            high_idx = 0
            low_idx = len(confidences) - 1
            mid_idx = len(confidences) // 2
            
            samples["high_confidence"] = confidences[high_idx][0] if high_idx < len(confidences) else None
            samples["medium_confidence"] = confidences[mid_idx][0] if mid_idx < len(confidences) else None
            samples["low_confidence"] = confidences[low_idx][0] if low_idx < len(confidences) else None
        
        return samples
    
    def generate_visualization(self, epoch, samples=None):
        if not self.val_images:
            return None
        
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        if samples is None:
            samples = self.select_representative_samples([])
        
        visualization_results = {
            "epoch": epoch,
            "samples": [],
            "output_dir": str(self.output_dir)
        }
        
        sample_list = []
        if isinstance(samples, dict):
            for category, path in samples.items():
                if path:
                    sample_list.append((category, path))
        else:
            for path in samples[:self.num_samples]:
                sample_list.append(("sample", path))
        
        for category, img_path in sample_list:
            try:
                results = self.model.predict(img_path, verbose=False)
                
                if results and len(results) > 0:
                    result = results[0]
                    
                    save_path = self.output_dir / f"epoch_{epoch}_{category}.jpg"
                    
                    try:
                        plotted = result.plot()
                        import cv2
                        cv2.imwrite(str(save_path), plotted)
                    except Exception as e:
                        print(f"âš ï¸ ä¿å­˜å¯è§†åŒ–å¤±è´¥: {e}", flush=True)
                    
                    sample_info = {
                        "category": category,
                        "image_path": img_path,
                        "output_path": str(save_path),
                        "num_detections": len(result.boxes) if result.boxes is not None else 0
                    }
                    
                    if result.boxes is not None and len(result.boxes) > 0:
                        confs = result.boxes.conf.cpu().numpy()
                        sample_info["avg_confidence"] = float(np.mean(confs))
                        sample_info["max_confidence"] = float(np.max(confs))
                        sample_info["min_confidence"] = float(np.min(confs))
                    
                    if hasattr(result, 'keypoints') and result.keypoints is not None:
                        kpts = result.keypoints
                        if hasattr(kpts, 'data') and kpts.data is not None:
                            sample_info["num_keypoints_detected"] = len(kpts.data)
                    
                    visualization_results["samples"].append(sample_info)
                    
            except Exception as e:
                print(f"âš ï¸ å¤„ç†å›¾ç‰‡ {img_path} å¤±è´¥: {e}", flush=True)
        
        return visualization_results

def get_per_keypoint_metrics(model, data_yaml, device='0'):
    try:
        import torch
        from ultralytics.utils.metrics import PoseMetricsStats
        
        print("ğŸ“Š å¼€å§‹è®¡ç®—å„å…³é”®ç‚¹è¯¯å·®åˆ†æ...", flush=True)
        
        val_results = model.val(
            data=data_yaml,
            device=device,
            verbose=False
        )
        
        keypoint_metrics = {
            "event": "per_keypoint_metrics",
            "keypoints": []
        }
        
        if hasattr(val_results, 'pose') and val_results.pose is not None:
            pose_metrics = val_results.pose
            
            if hasattr(pose_metrics, 'ap_per_class'):
                ap_per_class = pose_metrics.ap_per_class
                for i, ap in enumerate(ap_per_class):
                    keypoint_metrics["keypoints"].append({
                        "keypoint_id": i,
                        "ap": float(ap) if ap is not None else 0.0
                    })
        
        if hasattr(val_results, 'keypoints') and val_results.keypoints is not None:
            kpts = val_results.keypoints
            if hasattr(kpts, 'data'):
                for i, kpt_data in enumerate(kpts.data):
                    if len(keypoint_metrics["keypoints"]) > i:
                        keypoint_metrics["keypoints"][i]["visibility"] = float(kpt_data.mean()) if hasattr(kpt_data, 'mean') else 0.0
        
        try:
            results_dir = os.path.dirname(data_yaml)
            keypoint_error_file = os.path.join(results_dir, 'keypoint_errors.json')
            with open(keypoint_error_file, 'w') as f:
                json.dump(keypoint_metrics, f, indent=2)
        except:
            pass
        
        print(f"   åˆ†æäº† {len(keypoint_metrics['keypoints'])} ä¸ªå…³é”®ç‚¹", flush=True)
        
        return keypoint_metrics
        
    except Exception as e:
        print(f"âš ï¸ å…³é”®ç‚¹è¯¯å·®åˆ†æå¤±è´¥: {e}", flush=True)
        return {"event": "per_keypoint_metrics", "error": str(e), "keypoints": []}

gpu_monitor = None
visual_validator = None
performance_benchmark = None

def on_train_epoch_end(trainer):
    log_data = {
        "event": "epoch_end",
        "epoch": trainer.epoch + 1,
        "epochs": trainer.epochs,
        "totalEpochs": trainer.epochs,
    }

    if hasattr(trainer, 'loss_items') and trainer.loss_items is not None:
        loss_items = trainer.loss_items
        if len(loss_items) > 0:
            log_data["box_loss"] = float(loss_items[0])
        if len(loss_items) > 1:
            log_data["cls_loss"] = float(loss_items[1])
        if len(loss_items) > 2:
            log_data["dfl_loss"] = float(loss_items[2])
        if len(loss_items) > 3:
            log_data["pose_loss"] = float(loss_items[3])
        if len(loss_items) > 4:
            log_data["kobj_loss"] = float(loss_items[4])

    if hasattr(trainer, 'metrics') and trainer.metrics:
        if hasattr(trainer.metrics, 'box'):
            box = trainer.metrics.box
            log_data["box_precision"] = float(getattr(box, 'mp', 0))
            log_data["box_recall"] = float(getattr(box, 'mr', 0))
            log_data["mAP50"] = float(getattr(box, 'map50', 0))
            log_data["mAP50_95"] = float(getattr(box, 'map', 0))
        
        if hasattr(trainer.metrics, 'pose'):
            pose = trainer.metrics.pose
            log_data["pose_precision"] = float(getattr(pose, 'mp', 0))
            log_data["pose_recall"] = float(getattr(pose, 'mr', 0))
            log_data["pose_mAP50"] = float(getattr(pose, 'map50', 0))
            log_data["pose_mAP50_95"] = float(getattr(pose, 'map', 0))

    if hasattr(trainer, 'device') and trainer.device:
        log_data["gpu_mem"] = str(trainer.device)
    else:
        log_data["gpu_mem"] = "cpu"

    if hasattr(trainer, 'tloss') and trainer.tloss is not None:
        log_data["train_loss"] = float(trainer.tloss)
    
    if hasattr(trainer, 'optimizer') and trainer.optimizer:
        current_lr = None
        if hasattr(trainer.optimizer, 'param_groups') and len(trainer.optimizer.param_groups) > 0:
            current_lr = trainer.optimizer.param_groups[0].get('lr', 0)
        
        if current_lr is not None:
            log_data["learning_rate"] = float(current_lr)
        
        if hasattr(trainer, 'args'):
            args = trainer.args
            log_data["lr0"] = float(getattr(args, 'lr0', 0))
            log_data["lrf"] = float(getattr(args, 'lrf', 0))
            log_data["cos_lr"] = getattr(args, 'cos_lr', False)
    
    global gpu_monitor
    if gpu_monitor is not None:
        gpu_stats = gpu_monitor.get_gpu_stats()
        log_data["gpu_memory_used_gb"] = gpu_stats.get("gpu_memory_used_gb", 0)
        log_data["gpu_memory_total_gb"] = gpu_stats.get("gpu_memory_total_gb", 0)
        log_data["gpu_memory_percent"] = gpu_stats.get("gpu_memory_percent", 0)
        log_data["gpu_utilization_percent"] = gpu_stats.get("gpu_utilization_percent", 0)
        log_data["gpu_temperature"] = gpu_stats.get("gpu_temperature", 0)
        log_data["gpu_power_draw"] = gpu_stats.get("gpu_power_draw", 0)
        
        if gpu_stats.get("torch_memory_allocated_gb"):
            log_data["torch_memory_allocated_gb"] = gpu_stats["torch_memory_allocated_gb"]
        if gpu_stats.get("torch_memory_reserved_gb"):
            log_data["torch_memory_reserved_gb"] = gpu_stats["torch_memory_reserved_gb"]
        
        if gpu_stats.get("warnings"):
            log_data["gpu_warnings"] = gpu_stats["warnings"]

    log_json(log_data)
    
    global visual_validator
    if visual_validator is not None and (trainer.epoch + 1) % 5 == 0:
        try:
            viz_results = visual_validator.generate_visualization(trainer.epoch + 1)
            if viz_results:
                log_json({
                    "event": "visual_validation",
                    **viz_results
                })
        except Exception as e:
            print(f"âš ï¸ å¯è§†åŒ–éªŒè¯å¤±è´¥: {e}", flush=True)

def on_train_start(trainer):
    log_json({
        "event": "train_start",
        "model": trainer.args.model,
        "epochs": trainer.epochs,
        "batch": trainer.args.batch,
        "imgsz": trainer.args.imgsz
    })

def check_resume_available(args):
    """æ™ºèƒ½æ–­ç‚¹ç»­è®­æ£€æµ‹ï¼šæ£€æŸ¥æ˜¯å¦å­˜åœ¨å¯æ¢å¤çš„è®­ç»ƒ"""
    resume_info = {
        "available": False,
        "last_pt_path": None,
        "last_epoch": None,
        "best_epoch": None,
        "metrics": {}
    }
    
    try:
        project_dir = os.path.join(args.project, args.name) if args.project and args.name else None
        if not project_dir or not os.path.exists(project_dir):
            return resume_info
        
        weights_dir = os.path.join(project_dir, 'weights')
        if not os.path.exists(weights_dir):
            return resume_info
        
        last_pt_path = os.path.join(weights_dir, 'last.pt')
        if not os.path.exists(last_pt_path):
            return resume_info
        
        print(f"ğŸ” æ£€æµ‹åˆ°ä¸Šæ¬¡è®­ç»ƒ: {last_pt_path}", flush=True)
        
        try:
            checkpoint = torch.load(last_pt_path, map_location='cpu')
            
            resume_info["available"] = True
            resume_info["last_pt_path"] = last_pt_path
            
            if 'epoch' in checkpoint:
                resume_info["last_epoch"] = checkpoint['epoch']
                print(f"   ğŸ“Š ä¸Šæ¬¡è®­ç»ƒè½®æ¬¡: {checkpoint['epoch']}", flush=True)
            
            if 'best_epoch' in checkpoint:
                resume_info["best_epoch"] = checkpoint['best_epoch']
            
            if 'metrics' in checkpoint:
                resume_info["metrics"] = checkpoint['metrics']
                if 'best_map50' in checkpoint['metrics']:
                    print(f"   ğŸ“ˆ æœ€ä½³ mAP@50: {checkpoint['metrics']['best_map50']:.4f}", flush=True)
            
            if 'model' in checkpoint:
                model_info = checkpoint.get('model', {})
                if hasattr(model_info, 'args'):
                    saved_args = model_info.args if hasattr(model_info, 'args') else {}
                    resume_info["saved_config"] = {
                        "model": saved_args.get('model', 'unknown'),
                        "data": saved_args.get('data', 'unknown'),
                        "imgsz": saved_args.get('imgsz', 'unknown'),
                        "batch": saved_args.get('batch', 'unknown')
                    }
                    print(f"   âš™ï¸ ä¸Šæ¬¡é…ç½®: model={saved_args.get('model')}, imgsz={saved_args.get('imgsz')}", flush=True)
            
            log_json({
                "event": "resume_detected",
                "resume_info": resume_info
            })
            
        except Exception as e:
            print(f"   âš ï¸ è¯»å–æ£€æŸ¥ç‚¹å¤±è´¥: {e}", flush=True)
        
        return resume_info
        
    except Exception as e:
        return resume_info

def validate_config(args):
    """é£è¡Œå‰æ£€æŸ¥ï¼šéªŒè¯é…ç½®å®Œæ•´æ€§å’Œæ•°æ®æœ‰æ•ˆæ€§"""
    print("ğŸ” å¼€å§‹é£è¡Œå‰æ£€æŸ¥ (Pre-flight Check)...", flush=True)
    
    issues = []
    warnings = []
    checks_passed = {
        "yaml_file": False,
        "train_path": False,
        "val_path": False,
        "train_images": False,
        "val_images": False,
        "dependencies": False
    }
    
    # 1. YAML é…ç½®æ–‡ä»¶éªŒè¯
    print("  ğŸ“„ æ£€æŸ¥ YAML é…ç½®æ–‡ä»¶...", flush=True)
    try:
        import yaml
        
        if not os.path.exists(args.data):
            issues.append(f"YAML é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.data}")
        else:
            checks_passed["yaml_file"] = True
            with open(args.data, 'r', encoding='utf-8') as f:
                yaml_content = yaml.safe_load(f)
            
            if not yaml_content:
                issues.append("YAML é…ç½®æ–‡ä»¶ä¸ºç©º")
            else:
                # æ£€æŸ¥å¿…éœ€å­—æ®µ
                required_fields = ['path', 'train']
                for field in required_fields:
                    if field not in yaml_content:
                        issues.append(f"YAML ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
                
                if 'path' in yaml_content:
                    base_path = yaml_content['path']
                    
                    # 2. æ£€æŸ¥ train å’Œ val è·¯å¾„
                    if 'train' in yaml_content:
                        train_path = os.path.join(base_path, yaml_content['train']) if not os.path.isabs(yaml_content['train']) else yaml_content['train']
                        if os.path.exists(train_path):
                            checks_passed["train_path"] = True
                            print(f"     âœ… è®­ç»ƒé›†è·¯å¾„å­˜åœ¨: {train_path}", flush=True)
                        else:
                            issues.append(f"è®­ç»ƒé›†è·¯å¾„ä¸å­˜åœ¨: {train_path}")
                    
                    if 'val' in yaml_content:
                        val_path = os.path.join(base_path, yaml_content['val']) if not os.path.isabs(yaml_content['val']) else yaml_content['val']
                        if os.path.exists(val_path):
                            checks_passed["val_path"] = True
                            print(f"     âœ… éªŒè¯é›†è·¯å¾„å­˜åœ¨: {val_path}", flush=True)
                        else:
                            warnings.append(f"éªŒè¯é›†è·¯å¾„ä¸å­˜åœ¨: {val_path}")
                    
                    if 'test' in yaml_content:
                        test_path = os.path.join(base_path, yaml_content['test']) if not os.path.isabs(yaml_content['test']) else yaml_content['test']
                        if os.path.exists(test_path):
                            print(f"     âœ… æµ‹è¯•é›†è·¯å¾„å­˜åœ¨: {test_path}", flush=True)
                        else:
                            warnings.append(f"æµ‹è¯•é›†è·¯å¾„ä¸å­˜åœ¨: {test_path}")
                
                print("     âœ… YAML é…ç½®æ–‡ä»¶æ ¼å¼æ­£ç¡®", flush=True)
    except ImportError:
        issues.append("ç¼ºå°‘ pyyaml åº“ï¼Œè¯·è¿è¡Œ: pip install pyyaml")
    except yaml.YAMLError as e:
        issues.append(f"YAML è¯­æ³•é”™è¯¯: {e}")
    except Exception as e:
        issues.append(f"YAML è¯»å–é”™è¯¯: {e}")
    
    # 3. å›¾åƒæ•°æ®æœ‰æ•ˆæ€§æ£€æŸ¥
    print("  ğŸ–¼ï¸ æ£€æŸ¥å›¾åƒæ•°æ®æœ‰æ•ˆæ€§...", flush=True)
    
    def check_image_samples(directory, split_name, min_samples=5):
        if not os.path.exists(directory):
            return False, 0
        
        image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp')
        images = [f for f in os.listdir(directory) if f.lower().endswith(image_extensions)]
        
        if len(images) < min_samples:
            warnings.append(f"{split_name} é›†å›¾ç‰‡æ•°é‡è¾ƒå°‘: {len(images)} (å»ºè®®è‡³å°‘ {min_samples} å¼ )")
        
        # éšæœºæŠ½æ ·éªŒè¯å›¾ç‰‡å¯è¯»æ€§
        import random
        random.seed(42)
        
        sample_images = random.sample(images, min(len(images), min_samples)) if images else []
        
        try:
            from PIL import Image
            for img_file in sample_images:
                img_path = os.path.join(directory, img_file)
                try:
                    img = Image.open(img_path)
                    img.verify()
                except Exception as e:
                    issues.append(f"{split_name} é›†å›¾ç‰‡æŸå: {img_file} - {e}")
                    return False, len(images)
        except ImportError:
            warnings.append("PIL åº“æœªå®‰è£…ï¼Œè·³è¿‡å›¾ç‰‡å®Œæ•´æ€§éªŒè¯")
        
        return True, len(images)
    
    try:
        import yaml
        if os.path.exists(args.data):
            with open(args.data, 'r', encoding='utf-8') as f:
                yaml_content = yaml.safe_load(f)
            
            if yaml_content and 'path' in yaml_content:
                base_path = yaml_content['path']
                
                if 'train' in yaml_content:
                    train_dir = os.path.join(base_path, yaml_content['train']) if not os.path.isabs(yaml_content['train']) else yaml_content['train']
                    ok, count = check_image_samples(train_dir, "è®­ç»ƒ")
                    if ok:
                        checks_passed["train_images"] = True
                        print(f"     âœ… è®­ç»ƒé›†å›¾ç‰‡æ£€æŸ¥é€šè¿‡ ({count} å¼ )", flush=True)
                
                if 'val' in yaml_content:
                    val_dir = os.path.join(base_path, yaml_content['val']) if not os.path.isabs(yaml_content['val']) else yaml_content['val']
                    ok, count = check_image_samples(val_dir, "éªŒè¯")
                    if ok:
                        checks_passed["val_images"] = True
                        print(f"     âœ… éªŒè¯é›†å›¾ç‰‡æ£€æŸ¥é€šè¿‡ ({count} å¼ )", flush=True)
    except Exception as e:
        warnings.append(f"å›¾ç‰‡æ£€æŸ¥å‡ºé”™: {e}")
    
    # 4. ä¾èµ–åº“ç‰ˆæœ¬éªŒè¯
    print("  ğŸ“¦ æ£€æŸ¥ä¾èµ–åº“ç‰ˆæœ¬...", flush=True)
    
    def check_package_version(package_name, min_version=None):
        try:
            import importlib
            mod = importlib.import_module(package_name)
            version = getattr(mod, '__version__', 'unknown')
            return True, version
        except ImportError:
            return False, None
    
    required_packages = {
        'ultralytics': '8.0.0',
        'torch': '2.0.0',
        'cv2': '4.8.0',
        'PIL': '10.0.0',
        'numpy': '1.24.0',
        'yaml': '6.0',
        'pandas': '2.0.0',
        'matplotlib': '3.7.0'
    }
    
    all_deps_ok = True
    for pkg, min_ver in required_packages.items():
        ok, version = check_package_version(pkg)
        if ok:
            print(f"     âœ… {pkg}: {version}", flush=True)
        else:
            issues.append(f"ç¼ºå°‘å¿…éœ€åŒ…: {pkg} (éœ€è¦ç‰ˆæœ¬ >= {min_ver})")
            all_deps_ok = False
    
    if all_deps_ok:
        checks_passed["dependencies"] = True
    
    # æ£€æŸ¥ ultralytics ç‰¹å®šç‰ˆæœ¬
    try:
        from ultralytics import __version__ as ultralytics_version
        print(f"     ğŸ“Œ ultralytics ç‰ˆæœ¬: {ultralytics_version}", flush=True)
    except:
        pass
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "="*50, flush=True)
    print("ğŸ“‹ é£è¡Œå‰æ£€æŸ¥ç»“æœ:", flush=True)
    print("="*50, flush=True)
    
    passed_count = sum(checks_passed.values())
    total_count = len(checks_passed)
    
    for check_name, passed in checks_passed.items():
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"  {status}: {check_name}", flush=True)
    
    if warnings:
        print("\nâš ï¸ è­¦å‘Š:", flush=True)
        for w in warnings:
            print(f"  - {w}", flush=True)
    
    if issues:
        print("\nâŒ æ£€æŸ¥æœªé€šè¿‡:", flush=True)
        for issue in issues:
            print(f"  - {issue}", flush=True)
        print("\nğŸš« å¯åŠ¨ç»ˆæ­¢ï¼šé…ç½®éªŒè¯å¤±è´¥", flush=True)
        
        log_json({
            "event": "validation_failed",
            "passed": passed_count,
            "total": total_count,
            "issues": issues,
            "warnings": warnings
        })
        
        return False
    
    print(f"\nâœ… æ‰€æœ‰æ£€æŸ¥é€šè¿‡ ({passed_count}/{total_count})", flush=True)
    print("="*50 + "\n", flush=True)
    
    if warnings:
        log_json({
            "event": "validation_passed_with_warnings",
            "passed": passed_count,
            "total": total_count,
            "warnings": warnings
        })
    else:
        log_json({
            "event": "validation_passed",
            "passed": passed_count,
            "total": total_count
        })
    
    return True

def train_model(args):
    global gpu_monitor, visual_validator, performance_benchmark
    
    try:
        if not args.skip_validation:
            print("="*60, flush=True)
            print("ğŸš€ å¯åŠ¨è®­ç»ƒæµç¨‹", flush=True)
            print("="*60, flush=True)
            
            validation_ok = validate_config(args)
            if not validation_ok:
                sys.exit(1)
        else:
            print("âš ï¸ å·²è·³è¿‡é£è¡Œå‰æ£€æŸ¥ (--skip-validation)", flush=True)
        
        if not args.resume:
            resume_info = check_resume_available(args)
            if resume_info["available"]:
                print("\n" + "="*60, flush=True)
                print("ğŸ”„ æ£€æµ‹åˆ°å¯æ¢å¤çš„è®­ç»ƒ!", flush=True)
                print("="*60, flush=True)
                if resume_info["last_epoch"]:
                    print(f"   ä¸Šæ¬¡è®­ç»ƒè½®æ¬¡: {resume_info['last_epoch']}", flush=True)
                if resume_info["metrics"] and "best_map50" in resume_info["metrics"]:
                    print(f"   æœ€ä½³ mAP@50: {resume_info['metrics']['best_map50']:.4f}", flush=True)
                print("\nå¦‚éœ€ä»ä¸Šæ¬¡ä¸­æ–­å¤„ç»§ç»­è®­ç»ƒï¼Œè¯·æ·»åŠ  --resume å‚æ•°", flush=True)
                print("="*60 + "\n", flush=True)
        
        monitor_thread = threading.Thread(target=parent_monitor_thread, daemon=True)
        monitor_thread.start()
        
        hw_info = detect_hardware()
        log_json({
            "event": "hardware_check",
            "available": hw_info["available"],
            "gpu_count": hw_info["gpu_count"],
            "gpu_name": hw_info["gpu_name"],
            "cuda_version": hw_info["cuda_version"],
            "torch_cuda_version": hw_info["torch_cuda_version"],
            "total_memory_gb": hw_info.get("total_memory_gb", 0)
        })
        
        if args.device != 'cpu' and not hw_info["available"]:
            print("âš ï¸ è­¦å‘Š: æ‚¨æŒ‡å®šäº† GPU è®¾å¤‡ä½†ç³»ç»Ÿä¸æ”¯æŒ CUDA", flush=True)
            print("   å°†è‡ªåŠ¨åˆ‡æ¢åˆ° CPU æ¨¡å¼ç»§ç»­è®­ç»ƒ", flush=True)
            args.device = 'cpu'
        
        abs_data_path = os.path.abspath(args.data)
        if not os.path.exists(abs_data_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶: {abs_data_path}")

        print(f"ğŸš€ å¼€å§‹åŠ è½½æ¨¡å‹: {args.model}")
        print(f"ğŸ“‚ æ•°æ®é›†è·¯å¾„: {abs_data_path}")

        model = YOLO(args.model)
        
        device_id = 0
        if args.device != 'cpu':
            try:
                device_id = int(args.device)
            except:
                device_id = 0
        
        if hw_info["available"] and args.device != 'cpu':
            print("ğŸ”„ åˆå§‹åŒ–GPUç›‘æ§...", flush=True)
            gpu_monitor = GPUMonitor(device_id=device_id)
            gpu_monitor.start_monitoring()
            
            initial_stats = gpu_monitor.get_gpu_stats()
            log_json({
                "event": "gpu_monitor_started",
                "device_id": device_id,
                "gpu_name": hw_info.get("gpu_name"),
                "total_memory_gb": initial_stats.get("gpu_memory_total_gb", 0)
            })
        
        output_dir = os.path.join(args.project, args.name, "visualizations")
        visual_validator = VisualValidator(
            model=model,
            data_yaml=abs_data_path,
            output_dir=output_dir,
            num_samples=3
        )

        model.add_callback("on_train_start", on_train_start)
        model.add_callback("on_train_epoch_end", on_train_epoch_end)

        if args.resume:
            print("ğŸ”„ æ­£åœ¨æ¢å¤ä¸­æ–­çš„è®­ç»ƒ...")
            log_json({"event": "resume", "message": "Resuming training"})
            model.train(resume=True)
        else:
            augment_params = {
                'degrees': args.degrees,
                'translate': args.translate,
                'scale': args.scale,
                'shear': args.shear,
                'perspective': args.perspective,
                'fliplr': args.fliplr,
                'flipud': args.flipud,
                'hsv_h': args.hsv_h,
                'hsv_s': args.hsv_s,
                'hsv_v': args.hsv_v,
                'mosaic': args.mosaic,
                'mixup': args.mixup,
                'copy_paste': args.copy_paste,
                'erasing': args.erasing,
                'crop_fraction': args.crop_fraction,
            }

            print(f"ğŸ“Š æ•°æ®å¢å¼ºé…ç½®:")
            for k, v in augment_params.items():
                print(f"   {k}: {v}")

            training_params = {
                'data': abs_data_path,
                'epochs': args.epochs,
                'batch': args.batch,
                'imgsz': args.imgsz,
                'project': args.project,
                'name': args.name,
                'device': args.device,
                'workers': args.workers,
                'patience': args.patience,
                'optimizer': args.optimizer,
                'cos_lr': args.cos_lr,
                'rect': args.rect,
                **augment_params,
                'exist_ok': True,
                'verbose': True
            }

            if args.cache_images:
                training_params['cache'] = True
            if hasattr(args, 'close_mosaic') and args.close_mosaic > 0:
                training_params['close_mosaic'] = args.close_mosaic
            if hasattr(args, 'loss_pose'):
                training_params['pose'] = args.loss_pose
            if hasattr(args, 'loss_box'):
                training_params['box'] = args.loss_box
            if hasattr(args, 'loss_cls'):
                training_params['cls'] = args.loss_cls

            print(f"ğŸ“Š è®­ç»ƒé…ç½®:")
            for k, v in training_params.items():
                if k not in augment_params:
                    print(f"   {k}: {v}")

            results = model.train(**training_params)

        best_model_path = os.path.join(args.project, args.name, 'weights', 'best.pt')
        print(f"âœ… è®­ç»ƒå®Œæˆï¼æœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³: {best_model_path}")
        
        if gpu_monitor is not None:
            gpu_monitor.stop_monitoring()
            gpu_summary = gpu_monitor.get_summary()
            log_json({
                "event": "gpu_summary",
                **gpu_summary
            })
        
        print("â±ï¸ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•...", flush=True)
        performance_benchmark = PerformanceBenchmark(
            model=model,
            device=args.device,
            imgsz=args.imgsz
        )
        
        latency_results = performance_benchmark.measure_inference_latency(num_runs=30, warmup=3)
        throughput_results = performance_benchmark.measure_throughput(batch_sizes=[1, 2, 4], num_runs=20)
        
        perf_summary = performance_benchmark.get_summary()
        log_json({
            "event": "performance_benchmark",
            **perf_summary
        })
        
        print(f"   å®æ—¶FPS: {perf_summary['realtime_fps']} {'âœ… æ»¡è¶³å®æ—¶è¦æ±‚' if perf_summary['meets_realtime_requirement'] else 'âš ï¸ æœªè¾¾å®æ—¶è¦æ±‚'}")
        
        log_json({
            "event": "train_complete",
            "best_model": best_model_path
        })
        
        print("ğŸ” æ­£åœ¨æ‰§è¡Œæ¨¡å‹éªŒè¯...")
        validation_result = validate_model(model, args, best_model_path)
        
        print("ğŸ“Š æ­£åœ¨è®¡ç®—å…³é”®ç‚¹ç»†åˆ†è¯¯å·®...")
        keypoint_metrics = get_per_keypoint_metrics(model, abs_data_path, args.device)
        log_json(keypoint_metrics)
        
        if hasattr(args, 'export_formats') and args.export_formats:
            print("ğŸ“¦ æ­£åœ¨å¯¼å‡ºæ¨¡å‹...")
            export_results = export_model(model, args, best_model_path)
        
    except Exception as e:
        error_msg = str(e)
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", file=sys.stderr)
        
        if gpu_monitor is not None:
            gpu_monitor.stop_monitoring()
        
        friendly_error = format_user_friendly_error(error_msg)
        log_json(friendly_error)
        sys.exit(1)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Train YOLOv8-Pose for Fish Keypoints')

    parser.add_argument('--data', type=str, default='data.yaml', help='Path to data.yaml')
    parser.add_argument('--model', type=str, default='yolov8n-pose.pt', help='Base model')
    parser.add_argument('--epochs', type=int, default=150, help='Number of epochs')
    parser.add_argument('--batch', type=int, default=2, help='Batch size')
    parser.add_argument('--imgsz', type=int, default=1280, help='Image input size')

    parser.add_argument('--project', type=str, default='fish_run', help='Project directory')
    parser.add_argument('--name', type=str, default='exp_3', help='Experiment name')

    parser.add_argument('--device', type=str, default='0', help='Device (0, 1, 2 or cpu)')
    parser.add_argument('--workers', type=int, default=0, help='Dataloader workers')
    parser.add_argument('--cache_images', action='store_true', help='Cache images to memory')
    parser.add_argument('--patience', type=int, default=60, help='Early stopping patience')
    parser.add_argument('--cos_lr', action='store_true', help='Use cosine LR scheduler')
    parser.add_argument('--optimizer', type=str, default='auto', help='Optimizer (auto, SGD, Adam, AdamW)')
    parser.add_argument('--rect', action='store_true', help='Use rectangular training')

    parser.add_argument('--resume', action='store_true', help='Resume most recent training')

    parser.add_argument('--degrees', type=float, default=180.0, help='Rotation range')
    parser.add_argument('--translate', type=float, default=0.2, help='Translation fraction')
    parser.add_argument('--scale', type=float, default=0.6, help='Scale factor')
    parser.add_argument('--shear', type=float, default=0.0, help='Shear range')
    parser.add_argument('--perspective', type=float, default=0.001, help='Perspective distortion')

    parser.add_argument('--fliplr', type=float, default=0.5, help='Horizontal flip probability')
    parser.add_argument('--flipud', type=float, default=0.5, help='Vertical flip probability')

    parser.add_argument('--hsv_h', type=float, default=0.015, help='HSV Hue augmentation')
    parser.add_argument('--hsv_s', type=float, default=0.7, help='HSV Saturation augmentation')
    parser.add_argument('--hsv_v', type=float, default=0.4, help='HSV Value augmentation')

    parser.add_argument('--mosaic', type=float, default=0.0, help='Mosaic augmentation probability')
    parser.add_argument('--close_mosaic', type=int, default=0, help='Close mosaic in last N epochs')
    parser.add_argument('--mixup', type=float, default=0.0, help='MixUp augmentation probability')
    parser.add_argument('--copy_paste', type=float, default=0.0, help='Copy-paste augmentation probability')

    parser.add_argument('--erasing', type=float, default=0.4, help='Random erasing probability')
    parser.add_argument('--crop_fraction', type=float, default=1.0, help='Crop fraction')
    
    parser.add_argument('--skip_validation', action='store_true', help='Skip pre-flight validation check')
    
    parser.add_argument('--export_formats', type=str, default='', help='Auto-export formats after training (e.g., "onnx,tflite,torchscript")')

    parser.add_argument('--loss_pose', type=float, default=25.0, help='Pose loss weight')
    parser.add_argument('--loss_box', type=float, default=7.5, help='Box loss weight')
    parser.add_argument('--loss_cls', type=float, default=0.5, help='Class loss weight')

    args = parser.parse_args()

    train_model(args)
