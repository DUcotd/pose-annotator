import argparse
import sys
import os
import json
import time
import signal
import threading
import psutil

os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

from ultralytics import YOLO

PARENT_PID = os.getppid()
CHECK_INTERVAL = 10
should_stop = False

def log_json(data):
    print(f"__JSON_LOG__{json.dumps(data)}", flush=True)

def check_parent_alive():
    """å®šæœŸæ£€æŸ¥çˆ¶è¿›ç¨‹æ˜¯å¦å­˜æ´»ï¼Œå¦‚æœçˆ¶è¿›ç¨‹å·²é€€å‡ºåˆ™è‡ªåŠ¨ç»ˆæ­¢è‡ªèº«"""
    global should_stop
    try:
        if PARENT_PID == 1:
            print("âš ï¸ çˆ¶è¿›ç¨‹ä¸º init (PID=1)ï¼Œå‡è®¾çˆ¶è¿›ç¨‹å·²é€€å‡º", flush=True)
            should_stop = True
            return False
        
        try:
            parent = psutil.Process(PARENT_PID)
            if not parent.is_running():
                print(f"âš ï¸ çˆ¶è¿›ç¨‹ (PID={PARENT_PID}) ä¸å†è¿è¡Œï¼Œæ­£åœ¨é€€å‡º...", flush=True)
                should_stop = True
                return False
        except psutil.NoSuchProcess:
            print(f"âš ï¸ çˆ¶è¿›ç¨‹ (PID={PARENT_PID}) ä¸å­˜åœ¨ï¼Œæ­£åœ¨é€€å‡º...", flush=True)
            should_stop = True
            return False
        except psutil.AccessDenied:
            pass
            
        return True
    except Exception as e:
        print(f"âš ï¸ æ£€æŸ¥çˆ¶è¿›ç¨‹çŠ¶æ€æ—¶å‡ºé”™: {e}", flush=True)
        return True

def parent_monitor_thread():
    """åå°çº¿ç¨‹ï¼šå®šæœŸæ£€æŸ¥çˆ¶è¿›ç¨‹æ˜¯å¦å­˜æ´»"""
    global should_stop
    print(f"ğŸ”„ çˆ¶è¿›ç¨‹ç›‘æ§çº¿ç¨‹å·²å¯åŠ¨ (çˆ¶è¿›ç¨‹ PID: {PARENT_PID}, æ£€æŸ¥é—´éš”: {CHECK_INTERVAL}s)", flush=True)
    
    while not should_stop:
        time.sleep(CHECK_INTERVAL)
        if should_stop:
            break
        if not check_parent_alive():
            log_json({
                "event": "parent_exit",
                "message": "çˆ¶è¿›ç¨‹å·²é€€å‡ºï¼Œè®­ç»ƒå³å°†åœæ­¢"
            })
            break
    
    print("ğŸ”„ çˆ¶è¿›ç¨‹ç›‘æ§çº¿ç¨‹å·²é€€å‡º", flush=True)

def signal_handler(signum, frame):
    """å¤„ç†ç»ˆæ­¢ä¿¡å·"""
    global should_stop
    print(f"æ”¶åˆ°ä¿¡å· {signum}ï¼Œæ­£åœ¨åœæ­¢è®­ç»ƒ...", flush=True)
    should_stop = True

signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

ERROR_MAPPINGS = {
    "cuda": {
        "keywords": ["cuda", "CUDA", "GPU", "gpu device"],
        "type": "hardware",
        "title": "GPU ç›¸å…³é”™è¯¯",
        "suggestions": [
            "è¯·ç¡®è®¤å·²æ­£ç¡®å®‰è£… NVIDIA æ˜¾å¡é©±åŠ¨",
            "æ£€æŸ¥ PyTorch æ˜¯å¦æ”¯æŒ CUDA",
            "å°è¯•å°† device å‚æ•°æ”¹ä¸º 'cpu' ä½¿ç”¨ CPU æ¨¡å¼è®­ç»ƒ"
        ]
    },
    "cuda_oom": {
        "keywords": ["out of memory", "OOM", "CUDA out of memory", "RuntimeError: CUDA out of memory"],
        "type": "oom",
        "title": "æ˜¾å­˜ä¸è¶³ (OOM)",
        "suggestions": [
            "å‡å° batch_size å‚æ•°ï¼ˆå½“å‰å€¼å¯èƒ½è¿‡å¤§ï¼‰",
            "å‡å° imgsz å›¾ç‰‡å°ºå¯¸",
            "å°è¯•ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆå¦‚ yolov8n æˆ– yolov8sï¼‰",
            "å¼€å¯æ··åˆç²¾åº¦è®­ç»ƒå¯å‡å°‘æ˜¾å­˜å ç”¨"
        ]
    },
    "cudnn": {
        "keywords": ["cudnn", "CUDNN", "cuDNN"],
        "type": "cudnn",
        "title": "cuDNN é”™è¯¯",
        "suggestions": [
            "å¯èƒ½æ˜¯ CUDA ç‰ˆæœ¬ä¸ cuDNN ä¸åŒ¹é…",
            "å°è¯•æ›´æ–° NVIDIA é©±åŠ¨åˆ°æœ€æ–°ç‰ˆæœ¬",
            "æˆ–è®¾ç½®ç¯å¢ƒå˜é‡ CUDA_LAUNCH_BLOCKING=1 è·å–æ›´å¤šè°ƒè¯•ä¿¡æ¯"
        ]
    },
    "torch_cuda": {
        "keywords": ["torch.cuda", "torch is not able to use GPU"],
        "type": "torch_cuda",
        "title": "PyTorch æ— æ³•ä½¿ç”¨ GPU",
        "suggestions": [
            "ç¡®è®¤ PyTorch å·²æ­£ç¡®å®‰è£… CUDA ç‰ˆæœ¬",
            "æ‰§è¡Œ python -c \"import torch; print(torch.cuda.is_available())\" æ£€æŸ¥",
            "å°è¯•é‡æ–°å®‰è£… PyTorch: pip install torch --index-url https://download.pytorch.org/whl/cu118"
        ]
    },
    "no_gpu": {
        "keywords": ["No CUDA GPUs are available", "No GPU detected"],
        "type": "no_gpu",
        "title": "æœªæ£€æµ‹åˆ°å¯ç”¨ GPU",
        "suggestions": [
            "è¯·ç¡®è®¤ç”µè„‘å·²å®‰è£… NVIDIA æ˜¾å¡",
            "æ£€æŸ¥æ˜¾å¡é©±åŠ¨æ˜¯å¦æ­£ç¡®å®‰è£…",
            "åœ¨è®¾å¤‡ç®¡ç†å™¨ä¸­ç¡®è®¤æ˜¾å¡æœªè¢«ç¦ç”¨",
            "å¯ä½¿ç”¨ device: 'cpu' ä½¿ç”¨ CPU è¿›è¡Œè®­ç»ƒ"
        ]
    },
    "memory": {
        "keywords": ["MemoryError", "cannot allocate memory", "Unable to allocate"],
        "type": "memory",
        "title": "å†…å­˜ä¸è¶³",
        "suggestions": [
            "ç³»ç»Ÿå†…å­˜ä¸è¶³ï¼Œå°è¯•å…³é—­å…¶ä»–ç¨‹åº",
            "å‡å° batch_size å‚æ•°",
            "æ£€æŸ¥æ˜¯å¦æœ‰å†…å­˜æ³„æ¼"
        ]
    }
}

def detect_hardware():
    """è½»é‡çº§ç¡¬ä»¶ç¯å¢ƒæ£€æµ‹æ¨¡å—ï¼ŒéªŒè¯ GPU å¯ç”¨æ€§"""
    print("ğŸ” æ­£åœ¨æ£€æµ‹ç¡¬ä»¶ç¯å¢ƒ...", flush=True)
    
    result = {
        "available": False,
        "gpu_count": 0,
        "gpu_name": None,
        "cuda_version": None,
        "torch_cuda_version": None,
        "errors": []
    }
    
    try:
        import torch
        result["torch_cuda_version"] = torch.version.cuda if torch.version.cuda else None
        
        if torch.cuda.is_available():
            result["available"] = True
            result["gpu_count"] = torch.cuda.device_count()
            result["gpu_name"] = torch.cuda.get_device_name(0) if result["gpu_count"] > 0 else None
            result["cuda_version"] = torch.version.cuda
            
            total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            result["total_memory_gb"] = round(total_memory, 2)
            
            print(f"âœ… æ£€æµ‹åˆ° {result['gpu_count']} ä¸ª GPU", flush=True)
            print(f"   GPU 0: {result['gpu_name']}", flush=True)
            print(f"   CUDA ç‰ˆæœ¬: {result['cuda_version']}", flush=True)
            print(f"   æ˜¾å­˜æ€»é‡: {result['total_memory_gb']} GB", flush=True)
        else:
            result["errors"].append("CUDA ä¸å¯ç”¨")
            print("âš ï¸ CUDA ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ CPU æ¨¡å¼", flush=True)
            
    except ImportError:
        result["errors"].append("PyTorch æœªå®‰è£…")
        print("âš ï¸ PyTorch æœªå®‰è£…", flush=True)
    except Exception as e:
        result["errors"].append(str(e))
        print(f"âš ï¸ ç¡¬ä»¶æ£€æµ‹å¼‚å¸¸: {e}", flush=True)
    
    return result

def classify_error(error_msg):
    """æ ¹æ®é”™è¯¯ä¿¡æ¯åˆ†ç±»å¹¶è¿”å›å¤„ç†å»ºè®®"""
    error_lower = error_msg.lower()
    
    for key, mapping in ERROR_MAPPINGS.items():
        for keyword in mapping["keywords"]:
            if keyword.lower() in error_lower:
                return {
                    "type": mapping["type"],
                    "title": mapping["title"],
                    "suggestions": mapping["suggestions"],
                    "raw_error": error_msg[:500]
                }
    
    return {
        "type": "unknown",
        "title": "è®­ç»ƒé”™è¯¯",
        "suggestions": ["è¯·æ£€æŸ¥é…ç½®å‚æ•°æ˜¯å¦æ­£ç¡®", "æŸ¥çœ‹ä¸‹æ–¹åŸå§‹é”™è¯¯ä¿¡æ¯"],
        "raw_error": error_msg[:500]
    }

def export_model(model, args, model_path):
    """è®­ç»ƒå®Œæˆåè‡ªåŠ¨å¯¼å‡ºæ¨¡å‹åˆ°å¤šç§æ ¼å¼"""
    try:
        export_formats = args.export_formats.split(',') if isinstance(args.export_formats, str) else args.export_formats
        
        print(f"ğŸ“¦ å°†å¯¼å‡ºä»¥ä¸‹æ ¼å¼: {', '.join(export_formats)}")
        
        for pkg in ['pandas', 'matplotlib']:
            try:
                __import__(pkg)
            except ImportError:
                print(f"âš ï¸ è­¦å‘Š: {pkg} æœªå®‰è£…ï¼Œå¯¼å‡ºå’Œç»˜å›¾åŠŸèƒ½å¯èƒ½å—é™", flush=True)
                print(f"   å®‰è£…å‘½ä»¤: pip install {pkg}", flush=True)
        
        results = {
            "event": "export_start",
            "formats": export_formats,
            "base_model": model_path
        }
        
        exported_files = []
        failed_formats = []
        
        for fmt in export_formats:
            fmt = fmt.strip().lower()
            print(f"   æ­£åœ¨å¯¼å‡º {fmt} æ ¼å¼...", flush=True)
            
            try:
                export_path = model.export(format=fmt)
                
                if isinstance(export_path, str):
                    exported_files.append(export_path)
                    print(f"   âœ… {fmt} å¯¼å‡ºæˆåŠŸ: {export_path}", flush=True)
                else:
                    exported_files.append(str(export_path))
                    print(f"   âœ… {fmt} å¯¼å‡ºæˆåŠŸ", flush=True)
                    
            except Exception as e:
                failed_formats.append(fmt)
                print(f"   âŒ {fmt} å¯¼å‡ºå¤±è´¥: {e}", flush=True)
        
        print(f"\nâœ… æ¨¡å‹å¯¼å‡ºå®Œæˆ!", flush=True)
        
        final_result = {
            "event": "export_complete",
            "exported": exported_files,
            "failed": failed_formats,
            "total": len(export_formats),
            "success_count": len(exported_files),
            "failed_count": len(failed_formats)
        }
        
        log_json(final_result)
        
        if failed_formats:
            print(f"\nâš ï¸ ä»¥ä¸‹æ ¼å¼å¯¼å‡ºå¤±è´¥: {', '.join(failed_formats)}", flush=True)
        
        return final_result
        
    except Exception as e:
        print(f"âš ï¸ æ¨¡å‹å¯¼å‡ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", file=sys.stderr)
        log_json({
            "event": "export_error",
            "message": str(e),
            "base_model": model_path
        })
        return None

def validate_model(model, args, model_path):
    """è®­ç»ƒå®Œæˆåæ‰§è¡Œæ¨¡å‹éªŒè¯ï¼Œç”Ÿæˆè¯„ä¼°æŒ‡æ ‡ã€æ··æ·†çŸ©é˜µå’Œ PR æ›²çº¿"""
    try:
        val_results = model.val(
            data=args.data,
            batch=args.batch,
            imgsz=args.imgsz,
            device=args.device,
            save_json=True,
            save_hybrid=True,
            plots=True
        )
        
        print("âœ… æ¨¡å‹éªŒè¯å®Œæˆï¼")
        
        validation_data = {
            "event": "validation_complete",
            "model_path": model_path,
            "metrics": {}
        }
        
        # æå–ä¸»è¦è¯„ä¼°æŒ‡æ ‡
        if hasattr(val_results, 'box'):
            box_metrics = val_results.box
            validation_data["metrics"] = {
                "mAP50": float(getattr(box_metrics, 'map50', 0)),
                "mAP50-95": float(getattr(box_metrics, 'map', 0)),
                "precision": float(getattr(box_metrics, 'mp', 0)),
                "recall": float(getattr(box_metrics, 'mr', 0)),
                "f1": float(getattr(box_metrics, 'mf', 0)) if hasattr(box_metrics, 'mf') else 0.0
            }
        
        if hasattr(val_results, 'pose'):
            pose_metrics = val_results.pose
            validation_data["metrics"]["pose_mAP50"] = float(getattr(pose_metrics, 'map50', 0)) if hasattr(pose_metrics, 'map50') else 0.0
            validation_data["metrics"]["pose_mAP50-95"] = float(getattr(pose_metrics, 'map', 0)) if hasattr(pose_metrics, 'map') else 0.0
        
        # ç”Ÿæˆæ··æ·†çŸ©é˜µå’Œ PR æ›²çº¿çš„è·¯å¾„
        results_dir = os.path.join(args.project, args.name)
        
        confusion_matrix_path = os.path.join(results_dir, 'confusion_matrix.png')
        pr_curve_path = os.path.join(results_dir, 'PR_curve.png')
        
        validation_data["artifacts"] = {
            "confusion_matrix": confusion_matrix_path if os.path.exists(confusion_matrix_path) else None,
            "pr_curve": pr_curve_path if os.path.exists(pr_curve_path) else None
        }
        
        # æ‰“å°è¯„ä¼°æŒ‡æ ‡
        metrics = validation_data["metrics"]
        print(f"ğŸ“Š éªŒè¯æŒ‡æ ‡:")
        print(f"   mAP@50: {metrics.get('mAP50', 0):.4f}")
        print(f"   mAP@50-95: {metrics.get('mAP50-95', 0):.4f}")
        print(f"   Precision: {metrics.get('precision', 0):.4f}")
        print(f"   Recall: {metrics.get('recall', 0):.4f}")
        
        if metrics.get('pose_mAP50'):
            print(f"   Pose mAP@50: {metrics.get('pose_mAP50', 0):.4f}")
        
        # å‘é€éªŒè¯ç»“æœåˆ°å‰ç«¯
        log_json(validation_data)
        
        return validation_data
        
    except Exception as e:
        print(f"âš ï¸ æ¨¡å‹éªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", file=sys.stderr)
        log_json({
            "event": "validation_error",
            "message": str(e),
            "model_path": model_path
        })
        return None

def format_user_friendly_error(error_msg):
    """å°†æŠ€æœ¯é”™è¯¯è½¬æ¢ä¸ºç”¨æˆ·å‹å¥½çš„é”™è¯¯ä¿¡æ¯"""
    classified = classify_error(error_msg)
    
    return {
        "event": "error",
        "type": classified["type"],
        "title": classified["title"],
        "message": error_msg[:300],
        "suggestions": classified["suggestions"],
        "action": "è¯·æ ¹æ®å»ºè®®è°ƒæ•´åé‡è¯•"
    }

def on_train_epoch_end(trainer):
    log_data = {
        "event": "epoch_end",
        "epoch": trainer.epoch + 1,
        "epochs": trainer.epochs,
        "box_loss": float(trainer.loss_items[0]) if hasattr(trainer, 'loss_items') and len(trainer.loss_items) > 0 else 0.0,
        "cls_loss": float(trainer.loss_items[1]) if hasattr(trainer, 'loss_items') and len(trainer.loss_items) > 1 else 0.0,
        "dfl_loss": float(trainer.loss_items[2]) if hasattr(trainer, 'loss_items') and len(trainer.loss_items) > 2 else 0.0,
    }

    if hasattr(trainer, 'metrics') and trainer.metrics:
        if hasattr(trainer.metrics, 'box'):
            log_data["mAP50"] = float(getattr(trainer.metrics.box, 'map50', 0))
            log_data["mAP50-95"] = float(getattr(trainer.metrics.box, 'map', 0))

    if hasattr(trainer, 'device') and trainer.device:
        log_data["gpu_mem"] = str(trainer.device)
    else:
        log_data["gpu_mem"] = "cpu"

    log_json(log_data)

def on_train_start(trainer):
    log_json({
        "event": "train_start",
        "model": trainer.args.model,
        "epochs": trainer.epochs,
        "batch": trainer.args.batch,
        "imgsz": trainer.args.imgsz
    })

def check_resume_available(args):
    """æ™ºèƒ½æ–­ç‚¹ç»­è®­æ£€æµ‹ï¼šæ£€æŸ¥æ˜¯å¦å­˜åœ¨å¯æ¢å¤çš„è®­ç»ƒ"""
    resume_info = {
        "available": False,
        "last_pt_path": None,
        "last_epoch": None,
        "best_epoch": None,
        "metrics": {}
    }
    
    try:
        project_dir = os.path.join(args.project, args.name) if args.project and args.name else None
        if not project_dir or not os.path.exists(project_dir):
            return resume_info
        
        weights_dir = os.path.join(project_dir, 'weights')
        if not os.path.exists(weights_dir):
            return resume_info
        
        last_pt_path = os.path.join(weights_dir, 'last.pt')
        if not os.path.exists(last_pt_path):
            return resume_info
        
        print(f"ğŸ” æ£€æµ‹åˆ°ä¸Šæ¬¡è®­ç»ƒ: {last_pt_path}", flush=True)
        
        try:
            checkpoint = torch.load(last_pt_path, map_location='cpu')
            
            resume_info["available"] = True
            resume_info["last_pt_path"] = last_pt_path
            
            if 'epoch' in checkpoint:
                resume_info["last_epoch"] = checkpoint['epoch']
                print(f"   ğŸ“Š ä¸Šæ¬¡è®­ç»ƒè½®æ¬¡: {checkpoint['epoch']}", flush=True)
            
            if 'best_epoch' in checkpoint:
                resume_info["best_epoch"] = checkpoint['best_epoch']
            
            if 'metrics' in checkpoint:
                resume_info["metrics"] = checkpoint['metrics']
                if 'best_map50' in checkpoint['metrics']:
                    print(f"   ğŸ“ˆ æœ€ä½³ mAP@50: {checkpoint['metrics']['best_map50']:.4f}", flush=True)
            
            if 'model' in checkpoint:
                model_info = checkpoint.get('model', {})
                if hasattr(model_info, 'args'):
                    saved_args = model_info.args if hasattr(model_info, 'args') else {}
                    resume_info["saved_config"] = {
                        "model": saved_args.get('model', 'unknown'),
                        "data": saved_args.get('data', 'unknown'),
                        "imgsz": saved_args.get('imgsz', 'unknown'),
                        "batch": saved_args.get('batch', 'unknown')
                    }
                    print(f"   âš™ï¸ ä¸Šæ¬¡é…ç½®: model={saved_args.get('model')}, imgsz={saved_args.get('imgsz')}", flush=True)
            
            log_json({
                "event": "resume_detected",
                "resume_info": resume_info
            })
            
        except Exception as e:
            print(f"   âš ï¸ è¯»å–æ£€æŸ¥ç‚¹å¤±è´¥: {e}", flush=True)
        
        return resume_info
        
    except Exception as e:
        return resume_info

def validate_config(args):
    """é£è¡Œå‰æ£€æŸ¥ï¼šéªŒè¯é…ç½®å®Œæ•´æ€§å’Œæ•°æ®æœ‰æ•ˆæ€§"""
    print("ğŸ” å¼€å§‹é£è¡Œå‰æ£€æŸ¥ (Pre-flight Check)...", flush=True)
    
    issues = []
    warnings = []
    checks_passed = {
        "yaml_file": False,
        "train_path": False,
        "val_path": False,
        "train_images": False,
        "val_images": False,
        "dependencies": False
    }
    
    # 1. YAML é…ç½®æ–‡ä»¶éªŒè¯
    print("  ğŸ“„ æ£€æŸ¥ YAML é…ç½®æ–‡ä»¶...", flush=True)
    try:
        import yaml
        
        if not os.path.exists(args.data):
            issues.append(f"YAML é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.data}")
        else:
            checks_passed["yaml_file"] = True
            with open(args.data, 'r', encoding='utf-8') as f:
                yaml_content = yaml.safe_load(f)
            
            if not yaml_content:
                issues.append("YAML é…ç½®æ–‡ä»¶ä¸ºç©º")
            else:
                # æ£€æŸ¥å¿…éœ€å­—æ®µ
                required_fields = ['path', 'train']
                for field in required_fields:
                    if field not in yaml_content:
                        issues.append(f"YAML ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
                
                if 'path' in yaml_content:
                    base_path = yaml_content['path']
                    
                    # 2. æ£€æŸ¥ train å’Œ val è·¯å¾„
                    if 'train' in yaml_content:
                        train_path = os.path.join(base_path, yaml_content['train']) if not os.path.isabs(yaml_content['train']) else yaml_content['train']
                        if os.path.exists(train_path):
                            checks_passed["train_path"] = True
                            print(f"     âœ… è®­ç»ƒé›†è·¯å¾„å­˜åœ¨: {train_path}", flush=True)
                        else:
                            issues.append(f"è®­ç»ƒé›†è·¯å¾„ä¸å­˜åœ¨: {train_path}")
                    
                    if 'val' in yaml_content:
                        val_path = os.path.join(base_path, yaml_content['val']) if not os.path.isabs(yaml_content['val']) else yaml_content['val']
                        if os.path.exists(val_path):
                            checks_passed["val_path"] = True
                            print(f"     âœ… éªŒè¯é›†è·¯å¾„å­˜åœ¨: {val_path}", flush=True)
                        else:
                            warnings.append(f"éªŒè¯é›†è·¯å¾„ä¸å­˜åœ¨: {val_path}")
                    
                    if 'test' in yaml_content:
                        test_path = os.path.join(base_path, yaml_content['test']) if not os.path.isabs(yaml_content['test']) else yaml_content['test']
                        if os.path.exists(test_path):
                            print(f"     âœ… æµ‹è¯•é›†è·¯å¾„å­˜åœ¨: {test_path}", flush=True)
                        else:
                            warnings.append(f"æµ‹è¯•é›†è·¯å¾„ä¸å­˜åœ¨: {test_path}")
                
                print("     âœ… YAML é…ç½®æ–‡ä»¶æ ¼å¼æ­£ç¡®", flush=True)
    except ImportError:
        issues.append("ç¼ºå°‘ pyyaml åº“ï¼Œè¯·è¿è¡Œ: pip install pyyaml")
    except yaml.YAMLError as e:
        issues.append(f"YAML è¯­æ³•é”™è¯¯: {e}")
    except Exception as e:
        issues.append(f"YAML è¯»å–é”™è¯¯: {e}")
    
    # 3. å›¾åƒæ•°æ®æœ‰æ•ˆæ€§æ£€æŸ¥
    print("  ğŸ–¼ï¸ æ£€æŸ¥å›¾åƒæ•°æ®æœ‰æ•ˆæ€§...", flush=True)
    
    def check_image_samples(directory, split_name, min_samples=5):
        if not os.path.exists(directory):
            return False, 0
        
        image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp')
        images = [f for f in os.listdir(directory) if f.lower().endswith(image_extensions)]
        
        if len(images) < min_samples:
            warnings.append(f"{split_name} é›†å›¾ç‰‡æ•°é‡è¾ƒå°‘: {len(images)} (å»ºè®®è‡³å°‘ {min_samples} å¼ )")
        
        # éšæœºæŠ½æ ·éªŒè¯å›¾ç‰‡å¯è¯»æ€§
        import random
        random.seed(42)
        
        sample_images = random.sample(images, min(len(images), min_samples)) if images else []
        
        try:
            from PIL import Image
            for img_file in sample_images:
                img_path = os.path.join(directory, img_file)
                try:
                    img = Image.open(img_path)
                    img.verify()
                except Exception as e:
                    issues.append(f"{split_name} é›†å›¾ç‰‡æŸå: {img_file} - {e}")
                    return False, len(images)
        except ImportError:
            warnings.append("PIL åº“æœªå®‰è£…ï¼Œè·³è¿‡å›¾ç‰‡å®Œæ•´æ€§éªŒè¯")
        
        return True, len(images)
    
    try:
        import yaml
        if os.path.exists(args.data):
            with open(args.data, 'r', encoding='utf-8') as f:
                yaml_content = yaml.safe_load(f)
            
            if yaml_content and 'path' in yaml_content:
                base_path = yaml_content['path']
                
                if 'train' in yaml_content:
                    train_dir = os.path.join(base_path, yaml_content['train']) if not os.path.isabs(yaml_content['train']) else yaml_content['train']
                    ok, count = check_image_samples(train_dir, "è®­ç»ƒ")
                    if ok:
                        checks_passed["train_images"] = True
                        print(f"     âœ… è®­ç»ƒé›†å›¾ç‰‡æ£€æŸ¥é€šè¿‡ ({count} å¼ )", flush=True)
                
                if 'val' in yaml_content:
                    val_dir = os.path.join(base_path, yaml_content['val']) if not os.path.isabs(yaml_content['val']) else yaml_content['val']
                    ok, count = check_image_samples(val_dir, "éªŒè¯")
                    if ok:
                        checks_passed["val_images"] = True
                        print(f"     âœ… éªŒè¯é›†å›¾ç‰‡æ£€æŸ¥é€šè¿‡ ({count} å¼ )", flush=True)
    except Exception as e:
        warnings.append(f"å›¾ç‰‡æ£€æŸ¥å‡ºé”™: {e}")
    
    # 4. ä¾èµ–åº“ç‰ˆæœ¬éªŒè¯
    print("  ğŸ“¦ æ£€æŸ¥ä¾èµ–åº“ç‰ˆæœ¬...", flush=True)
    
    def check_package_version(package_name, min_version=None):
        try:
            import importlib
            mod = importlib.import_module(package_name)
            version = getattr(mod, '__version__', 'unknown')
            return True, version
        except ImportError:
            return False, None
    
    required_packages = {
        'ultralytics': '8.0.0',
        'torch': '2.0.0',
        'cv2': '4.8.0',
        'PIL': '10.0.0',
        'numpy': '1.24.0',
        'yaml': '6.0',
        'pandas': '2.0.0',
        'matplotlib': '3.7.0'
    }
    
    all_deps_ok = True
    for pkg, min_ver in required_packages.items():
        ok, version = check_package_version(pkg)
        if ok:
            print(f"     âœ… {pkg}: {version}", flush=True)
        else:
            issues.append(f"ç¼ºå°‘å¿…éœ€åŒ…: {pkg} (éœ€è¦ç‰ˆæœ¬ >= {min_ver})")
            all_deps_ok = False
    
    if all_deps_ok:
        checks_passed["dependencies"] = True
    
    # æ£€æŸ¥ ultralytics ç‰¹å®šç‰ˆæœ¬
    try:
        from ultralytics import __version__ as ultralytics_version
        print(f"     ğŸ“Œ ultralytics ç‰ˆæœ¬: {ultralytics_version}", flush=True)
    except:
        pass
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "="*50, flush=True)
    print("ğŸ“‹ é£è¡Œå‰æ£€æŸ¥ç»“æœ:", flush=True)
    print("="*50, flush=True)
    
    passed_count = sum(checks_passed.values())
    total_count = len(checks_passed)
    
    for check_name, passed in checks_passed.items():
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"  {status}: {check_name}", flush=True)
    
    if warnings:
        print("\nâš ï¸ è­¦å‘Š:", flush=True)
        for w in warnings:
            print(f"  - {w}", flush=True)
    
    if issues:
        print("\nâŒ æ£€æŸ¥æœªé€šè¿‡:", flush=True)
        for issue in issues:
            print(f"  - {issue}", flush=True)
        print("\nğŸš« å¯åŠ¨ç»ˆæ­¢ï¼šé…ç½®éªŒè¯å¤±è´¥", flush=True)
        
        log_json({
            "event": "validation_failed",
            "passed": passed_count,
            "total": total_count,
            "issues": issues,
            "warnings": warnings
        })
        
        return False
    
    print(f"\nâœ… æ‰€æœ‰æ£€æŸ¥é€šè¿‡ ({passed_count}/{total_count})", flush=True)
    print("="*50 + "\n", flush=True)
    
    if warnings:
        log_json({
            "event": "validation_passed_with_warnings",
            "passed": passed_count,
            "total": total_count,
            "warnings": warnings
        })
    else:
        log_json({
            "event": "validation_passed",
            "passed": passed_count,
            "total": total_count
        })
    
    return True

def train_model(args):
    try:
        # æ­¥éª¤ 0: é£è¡Œå‰æ£€æŸ¥ï¼ˆå¯é€‰è·³è¿‡ï¼‰
        if not args.skip_validation:
            print("="*60, flush=True)
            print("ğŸš€ å¯åŠ¨è®­ç»ƒæµç¨‹", flush=True)
            print("="*60, flush=True)
            
            validation_ok = validate_config(args)
            if not validation_ok:
                sys.exit(1)
        else:
            print("âš ï¸ å·²è·³è¿‡é£è¡Œå‰æ£€æŸ¥ (--skip-validation)", flush=True)
        
        # æ­¥éª¤ 0.5: æ™ºèƒ½æ–­ç‚¹ç»­è®­æ£€æµ‹
        if not args.resume:
            resume_info = check_resume_available(args)
            if resume_info["available"]:
                print("\n" + "="*60, flush=True)
                print("ğŸ”„ æ£€æµ‹åˆ°å¯æ¢å¤çš„è®­ç»ƒ!", flush=True)
                print("="*60, flush=True)
                if resume_info["last_epoch"]:
                    print(f"   ä¸Šæ¬¡è®­ç»ƒè½®æ¬¡: {resume_info['last_epoch']}", flush=True)
                if resume_info["metrics"] and "best_map50" in resume_info["metrics"]:
                    print(f"   æœ€ä½³ mAP@50: {resume_info['metrics']['best_map50']:.4f}", flush=True)
                print("\nå¦‚éœ€ä»ä¸Šæ¬¡ä¸­æ–­å¤„ç»§ç»­è®­ç»ƒï¼Œè¯·æ·»åŠ  --resume å‚æ•°", flush=True)
                print("="*60 + "\n", flush=True)
        
        # å¯åŠ¨çˆ¶è¿›ç¨‹ç›‘æ§çº¿ç¨‹
        monitor_thread = threading.Thread(target=parent_monitor_thread, daemon=True)
        monitor_thread.start()
        
        # æ­¥éª¤ 1: ç¡¬ä»¶ç¯å¢ƒé¢„æ£€æµ‹
        hw_info = detect_hardware()
        log_json({
            "event": "hardware_check",
            "available": hw_info["available"],
            "gpu_count": hw_info["gpu_count"],
            "gpu_name": hw_info["gpu_name"],
            "cuda_version": hw_info["cuda_version"],
            "torch_cuda_version": hw_info["torch_cuda_version"]
        })
        
        # æ­¥éª¤ 2: å¦‚æœç”¨æˆ·æŒ‡å®šäº† GPU ä½†ç¡¬ä»¶ä¸æ”¯æŒï¼Œç»™å‡ºè­¦å‘Š
        if args.device != 'cpu' and not hw_info["available"]:
            print("âš ï¸ è­¦å‘Š: æ‚¨æŒ‡å®šäº† GPU è®¾å¤‡ä½†ç³»ç»Ÿä¸æ”¯æŒ CUDA", flush=True)
            print("   å°†è‡ªåŠ¨åˆ‡æ¢åˆ° CPU æ¨¡å¼ç»§ç»­è®­ç»ƒ", flush=True)
            args.device = 'cpu'
        
        abs_data_path = os.path.abspath(args.data)
        if not os.path.exists(abs_data_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶: {abs_data_path}")

        print(f"ğŸš€ å¼€å§‹åŠ è½½æ¨¡å‹: {args.model}")
        print(f"ğŸ“‚ æ•°æ®é›†è·¯å¾„: {abs_data_path}")

        model = YOLO(args.model)

        model.add_callback("on_train_start", on_train_start)
        model.add_callback("on_train_epoch_end", on_train_epoch_end)

        if args.resume:
            print("ğŸ”„ æ­£åœ¨æ¢å¤ä¸­æ–­çš„è®­ç»ƒ...")
            log_json({"event": "resume", "message": "Resuming training"})
            model.train(resume=True)
        else:
            augment_params = {
                'degrees': args.degrees,
                'translate': args.translate,
                'scale': args.scale,
                'shear': args.shear,
                'perspective': args.perspective,
                'fliplr': args.fliplr,
                'flipud': args.flipud,
                'hsv_h': args.hsv_h,
                'hsv_s': args.hsv_s,
                'hsv_v': args.hsv_v,
                'mosaic': args.mosaic,
                'mixup': args.mixup,
                'copy_paste': args.copy_paste,
                'erasing': args.erasing,
                'crop_fraction': args.crop_fraction,
            }

            print(f"ğŸ“Š æ•°æ®å¢å¼ºé…ç½®:")
            for k, v in augment_params.items():
                print(f"   {k}: {v}")

            training_params = {
                'data': abs_data_path,
                'epochs': args.epochs,
                'batch': args.batch,
                'imgsz': args.imgsz,
                'project': args.project,
                'name': args.name,
                'device': args.device,
                'workers': args.workers,
                'patience': args.patience,
                'optimizer': args.optimizer,
                'cos_lr': args.cos_lr,
                'rect': args.rect,
                **augment_params,
                'exist_ok': True,
                'verbose': True
            }

            if args.cache_images:
                training_params['cache'] = True
            if hasattr(args, 'close_mosaic') and args.close_mosaic > 0:
                training_params['close_mosaic'] = args.close_mosaic
            if hasattr(args, 'loss_pose'):
                training_params['pose'] = args.loss_pose
            if hasattr(args, 'loss_box'):
                training_params['box'] = args.loss_box
            if hasattr(args, 'loss_cls'):
                training_params['cls'] = args.loss_cls

            print(f"ğŸ“Š è®­ç»ƒé…ç½®:")
            for k, v in training_params.items():
                if k not in augment_params:
                    print(f"   {k}: {v}")

            results = model.train(**training_params)

        best_model_path = os.path.join(args.project, args.name, 'weights', 'best.pt')
        print(f"âœ… è®­ç»ƒå®Œæˆï¼æœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³: {best_model_path}")
        log_json({
            "event": "train_complete",
            "best_model": best_model_path
        })
        
        # è®­ç»ƒå®Œæˆåæ‰§è¡Œæ¨¡å‹éªŒè¯
        print("ğŸ” æ­£åœ¨æ‰§è¡Œæ¨¡å‹éªŒè¯...")
        validation_result = validate_model(model, args, best_model_path)
        
        # è®­ç»ƒå®Œæˆåè‡ªåŠ¨å¯¼å‡ºæ¨¡å‹
        if hasattr(args, 'export_formats') and args.export_formats:
            print("ğŸ“¦ æ­£åœ¨å¯¼å‡ºæ¨¡å‹...")
            export_results = export_model(model, args, best_model_path)
        
    except Exception as e:
        error_msg = str(e)
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", file=sys.stderr)
        
        # ä½¿ç”¨å‹å¥½çš„é”™è¯¯æ ¼å¼
        friendly_error = format_user_friendly_error(error_msg)
        log_json(friendly_error)
        sys.exit(1)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Train YOLOv8-Pose for Fish Keypoints')

    parser.add_argument('--data', type=str, default='data.yaml', help='Path to data.yaml')
    parser.add_argument('--model', type=str, default='yolov8s-pose.pt', help='Base model')
    parser.add_argument('--epochs', type=int, default=150, help='Number of epochs')
    parser.add_argument('--batch', type=int, default=2, help='Batch size')
    parser.add_argument('--imgsz', type=int, default=1280, help='Image input size')

    parser.add_argument('--project', type=str, default='fish_run', help='Project directory')
    parser.add_argument('--name', type=str, default='exp_3', help='Experiment name')

    parser.add_argument('--device', type=str, default='0', help='Device (0, 1, 2 or cpu)')
    parser.add_argument('--workers', type=int, default=0, help='Dataloader workers')
    parser.add_argument('--cache_images', action='store_true', help='Cache images to memory')
    parser.add_argument('--patience', type=int, default=60, help='Early stopping patience')
    parser.add_argument('--cos_lr', action='store_true', help='Use cosine LR scheduler')
    parser.add_argument('--optimizer', type=str, default='auto', help='Optimizer (auto, SGD, Adam, AdamW)')
    parser.add_argument('--rect', action='store_true', help='Use rectangular training')

    parser.add_argument('--resume', action='store_true', help='Resume most recent training')

    parser.add_argument('--degrees', type=float, default=180.0, help='Rotation range')
    parser.add_argument('--translate', type=float, default=0.2, help='Translation fraction')
    parser.add_argument('--scale', type=float, default=0.6, help='Scale factor')
    parser.add_argument('--shear', type=float, default=0.0, help='Shear range')
    parser.add_argument('--perspective', type=float, default=0.001, help='Perspective distortion')

    parser.add_argument('--fliplr', type=float, default=0.5, help='Horizontal flip probability')
    parser.add_argument('--flipud', type=float, default=0.5, help='Vertical flip probability')

    parser.add_argument('--hsv_h', type=float, default=0.015, help='HSV Hue augmentation')
    parser.add_argument('--hsv_s', type=float, default=0.7, help='HSV Saturation augmentation')
    parser.add_argument('--hsv_v', type=float, default=0.4, help='HSV Value augmentation')

    parser.add_argument('--mosaic', type=float, default=0.0, help='Mosaic augmentation probability')
    parser.add_argument('--close_mosaic', type=int, default=0, help='Close mosaic in last N epochs')
    parser.add_argument('--mixup', type=float, default=0.0, help='MixUp augmentation probability')
    parser.add_argument('--copy_paste', type=float, default=0.0, help='Copy-paste augmentation probability')

    parser.add_argument('--erasing', type=float, default=0.4, help='Random erasing probability')
    parser.add_argument('--crop_fraction', type=float, default=1.0, help='Crop fraction')
    
    parser.add_argument('--skip_validation', action='store_true', help='Skip pre-flight validation check')
    
    parser.add_argument('--export_formats', type=str, default='', help='Auto-export formats after training (e.g., "onnx,tflite,torchscript")')

    parser.add_argument('--loss_pose', type=float, default=25.0, help='Pose loss weight')
    parser.add_argument('--loss_box', type=float, default=7.5, help='Box loss weight')
    parser.add_argument('--loss_cls', type=float, default=0.5, help='Class loss weight')

    args = parser.parse_args()

    train_model(args)
